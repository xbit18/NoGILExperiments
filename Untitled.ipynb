{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8b677fb-98a7-45ac-8457-a5fbfb4f21e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8e1816e6-1291-4d21-b790-8b293a1634d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "import sys\n",
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from pprint import pprint as pp\n",
    "import matplotlib.pyplot as plt\n",
    "import subprocess\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "674bae37-daeb-4668-acc0-1792e5d4f020",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Benchmarks</th>\n",
       "      <th>prova</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>async_generators</td>\n",
       "      <td>25.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>async_tree_cpu_io_mixed</td>\n",
       "      <td>72.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>async_tree_cpu_io_mixed_tg</td>\n",
       "      <td>77.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>async_tree_eager</td>\n",
       "      <td>13.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>async_tree_eager_cpu_io_mixed</td>\n",
       "      <td>29.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>unpickle_pure_python</td>\n",
       "      <td>8.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>xml_etree_generate</td>\n",
       "      <td>11.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>xml_etree_iterparse</td>\n",
       "      <td>10.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>xml_etree_parse</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>xml_etree_process</td>\n",
       "      <td>13.4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>103 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Benchmarks  prova\n",
       "0                 async_generators   25.4\n",
       "1          async_tree_cpu_io_mixed   72.5\n",
       "2       async_tree_cpu_io_mixed_tg   77.8\n",
       "3                 async_tree_eager   13.2\n",
       "4    async_tree_eager_cpu_io_mixed   29.3\n",
       "..                             ...    ...\n",
       "98            unpickle_pure_python    8.7\n",
       "99              xml_etree_generate   11.2\n",
       "100            xml_etree_iterparse   10.5\n",
       "101                xml_etree_parse   13.0\n",
       "102              xml_etree_process   13.4\n",
       "\n",
       "[103 rows x 2 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = './pyperf_res/memory'\n",
    "files_to_process = []\n",
    "for file_name in os.listdir(path):\n",
    "    if file_name.endswith('.json'):\n",
    "        files_to_process.append(file_name)\n",
    "\n",
    "processed_files = {}\n",
    "for file in files_to_process:\n",
    "    f = open(path + file)\n",
    "    data = json.load(f)\n",
    "    benchmarks = {}\n",
    "    for d in data['benchmarks'][1:]:\n",
    "        benchmarks[d['metadata']['name']] = d['runs'][1:]\n",
    "    processed_files[f\"{file.replace('.json', '')}_processed.json\"] = benchmarks\n",
    "\n",
    "columns = {\n",
    "    'py310_processed.json': '3.10.13',\n",
    "    'py311_processed.json': '3.11.8',\n",
    "    'py312_processed.json': '3.12.2',\n",
    "    'py3120_processed.json': '3.12.0',\n",
    "    'py39_processed.json': '3.9.18',\n",
    "    'py3910_processed.json': '3.9.10',\n",
    "    'pynogil_processed.json': 'nogil-3.9.10',\n",
    "    'memory_processed.json': 'prova'\n",
    "}\n",
    "\n",
    "# Get complete list of benchmarks\n",
    "benchmarks = []\n",
    "for file in sorted(processed_files):\n",
    "    benchmarks.extend(processed_files[file].keys())\n",
    "\n",
    "benchmarks = sorted(list(set(benchmarks)))\n",
    "df = {}\n",
    "df['Benchmarks'] = benchmarks\n",
    "for file in sorted(processed_files):\n",
    "    all_mems = []\n",
    "    data = processed_files[file]\n",
    "    for key in benchmarks:\n",
    "        if data.get(key):\n",
    "            bench = data[key]\n",
    "            mems = []\n",
    "            for run in bench:\n",
    "                vals = run['values']\n",
    "                mems.extend(vals)\n",
    "            # print(times)\n",
    "            avg_mem = np.average(mems)\n",
    "            avg_mem = avg_mem/1024/1024\n",
    "            all_mems.append(round(avg_mem, 1))\n",
    "        else:\n",
    "            all_mems.append(np.nan)\n",
    "    df[columns[file].replace('.json', '')] = all_mems\n",
    "    \n",
    "mems_df = pd.DataFrame(df)\n",
    "columns = ['Benchmarks', '3.9.10', 'nogil-3.9.10', '3.9.18', '3.10.13', '3.11.8', '3.12.0', '3.12.2']\n",
    "mems_df = mems_df[columns]\n",
    "mems_df_notnull = mems_df.dropna()\n",
    "mems_df_notnull.reset_index(inplace=True, drop=True)\n",
    "mems_df = mems_df_notnull\n",
    "\n",
    "mems_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f40fbc-6130-4869-9e7e-5cca225053e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "mems = []\n",
    "for col in mems_df.columns[1:]:\n",
    "    avg_mem = np.average(mems_df[col])\n",
    "    mems.append(avg_mem)\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "labels = list(mems_df.columns)[1:]\n",
    "\n",
    "colors = [\"#ff1500\", \"#ff9602\", \"#f5cc02\", \"#00d200\", \"#00c3ff\", \"#0022ff\", \"#b700ff\"]\n",
    "colors.reverse()\n",
    "for i in range(len(labels)):\n",
    "    plt.bar(i, mems[i], color=colors.pop())\n",
    "plt.xlabel(\"Python Versions\")\n",
    "plt.ylabel(\"Avg Execution Time\")\n",
    "plt.legend(labels)\n",
    "ticks = [i for i in range(len(mems))]\n",
    "plt.xticks(ticks, labels=labels)\n",
    "plt.savefig(f\"./images/confronto_single_thread_memoria.png\", bbox_inches='tight', transparent=False, pad_inches=0.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940cc699-ca5b-4fe2-b08a-6de9140b8ffc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
